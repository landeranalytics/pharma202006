---
title: "ML in R"
author: "Jared P. Lander"
date: "6/25/2020"
output: 
    html_document:
        toc: true
        toc_float:
            smooth_scroll: true
            collapsed: false
        number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data

```{r}
piggyback::pb_download(file='data/med_costs.csv', repo='jaredlander/coursedata')
```

```{r}
here::here('data', 'med_costs.csv')
med <- readr::read_csv(here::here('data', 'med_costs.csv'))
```

```{r}
med
```

```{r}
library(ggplot2)
ggplot(med, aes(x=age, fill=sex)) + geom_histogram() + facet_wrap(~sex) + theme(legend.position='none')
ggplot(med, aes(x=age, y=charges, color=sex)) + 
    geom_point() + 
    geom_smooth() + 
    facet_wrap(~sex) + 
    theme(legend.position='none')

ggplot(med, aes(x=age, y=charges)) + 
    geom_point(aes(color=region)) + 
    geom_smooth() + 
    facet_wrap(~sex)

ggplot(med, aes(x=age, y=charges)) + 
    geom_point(aes(color=smoker)) + 
    geom_smooth() + 
    facet_wrap(~sex)
```

# Regression

$$
y \sim a + b_1x_1 + b_2x_2 + \cdots + b_px_p + \epsilon \\
\epsilon \sim N(0, \sigma)
$$

- Output: response, outcome, dependent variable, y, label, target
- Input: predictor, x, variable, covariate, feature, independent variable, data

## lm

```{r}
cost1 <- lm(charges ~ age + sex + smoker, data=med)
summary(cost1)
```

```{r}
library(coefplot)
coefplot(cost1, sort='magnitude')
coefplot(cost1, sort='magnitude', lwdInner=1.5, lwdOuter=0.5)
```

```{r}
cost2 <- lm(charges ~ scale(age) + sex + smoker, data=med)
coefplot(cost2, sort='magnitude')
```

```{r}
med
```

```{r}
2^6 - 1
2^10 - 1
2^25 - 1
2^120 - 1
```

## Lasso

$$
\hat{\beta} = argmin \left [ \sum (y_i - X_i\beta)^2 + \lambda \sum |\beta_j| \right ]
$$
```{r}
library(glmnet)
```

Is it pronounced "glmnet" or "g-l-m-net"?

glmnet is the winner

```{r eval=FALSE}
# this code is not run
# it would not work anyway
glmnet(y ~ x1 + x2, data=data)
```

Starting with glmnet 4.0 (possibly 3.0), you can use a formula

So we need to build our own matrices

Used to `useful::build.x()`

Now I use `{recipes}`

```{r}
library(recipes)

rec_glmnet1 <- recipe(charges ~ ., data=med) %>% 
    step_knnimpute(all_predictors()) %>% 
    # step_normalize(age, BMI) %>% 
    step_normalize(all_numeric(), -charges) %>%
    # step_center(all_numeric(), -charges) %>%
    # step_scale(all_numeric(), -charges, factor=2) %>%
    step_dummy(all_nominal(), one_hot=TRUE)
rec_glmnet1
```

```{r}
prepped_glmnet1 <- rec_glmnet1 %>% prep()
prepped_glmnet1
```

```{r}
prepped_glmnet1 %>% juice()
```

```{r}
prepped_glmnet1 %>% juice() %>% dplyr::select(-charges) %>% as.matrix %>% head
prepped_glmnet1 %>% juice(all_predictors(), composition='matrix') %>% head
```

Dense matrix: every number is represented in a 2D array
Sparse matrix: does not store 0s, and the data is stored in a three-column array where the first two columns show the row and column number being represented, and the third column shows the value

Sparse matrices take up less space in memory and are faster to do math

```{r}
x_glm <- prepped_glmnet1 %>% juice(all_predictors(), composition='dgCMatrix')
x_glm
image(x_glm[1:20, 1:11])
```

dgC standards for double (numeric) compressed sparse column

```{r}
y_glm <- prepped_glmnet1 %>% juice(all_outcomes(), composition='matrix')
head(y_glm)
```

```{r}
cost3 <- glmnet(x=x_glm, y=y_glm, family='gaussian')
```

```{r}
coef(cost3)
# View(as.matrix(coef(cost3)))
```


```{r}
plot(cost3, xvar='lambda')
plot(cost3, xvar='lambda', label=TRUE)
coefpath(cost3)
```

```{r}
cost4 <- cv.glmnet(x=x_glm, y=y_glm, family='gaussian', nfolds=10)
```

```{r}
plot(cost4)
```

```{r}
cost4$lambda.min
cost4$lambda.1se
```

```{r}
coefpath(cost4)
coefplot(cost4, sort='magnitude', lambda='lambda.min')
coefplot(cost4, sort='magnitude', lambda='lambda.1se')
coefplot(cost4, sort='magnitude', lambda=456)
```

## Ridge

$$
\hat{\beta} = argmin \left [ \sum (y_i - X_i\beta)^2 + \lambda \sum |\beta_j|^2 \right ]
$$
```{r}
cost5 <- cv.glmnet(x=x_glm, y=y_glm, family='gaussian', nfolds=10, alpha=0)
```

```{r}
coefpath(cost5)
plot(cost5$glmnet.fit, xvar='lambda')
```

## Elastic Net

The elastic net is a combination of lasso and ridge

$$
\hat{\beta} = argmin \left [ \sum (y_i - X_i\beta)^2 + \lambda\left( \frac{1}{2}\alpha \sum |\beta_j| +  (1-\alpha)\sum |\beta_j|^2 \right) \right ]
$$

```{r}
cost6 <- cv.glmnet(x=x_glm, y=y_glm, family='gaussian', nfolds=10, alpha=0.6)
```

```{r}
coefpath(cost6)
coefplot(cost6, sort='magnitude', lambda='lambda.min')
```


```{r}
seq(0, 1, by=0.1)
```

Lasso was invented for when you have many more columns than rows

Guaranteed to select no more columns than the number of rows

## Predictions

This is wrong, but we'll make a fake test set

We should have done this in the beginning, but for now, we just want to show how to predict, so please forgive the transgression

```{r}
set.seed(7561)
fake_test_data <- med %>% dplyr::slice_sample(n=10)
fake_test_data
```

We can't just predict from a data.frame, we need a matrix

```{r}
x_test <- prepped_glmnet1 %>% bake(all_predictors(), composition='dgCMatrix', new_data=fake_test_data)
x_test
```

```{r}
preds_glm6 <- predict(cost6, newx=x_test, s='lambda.1se')
preds_glm6
```

```{r}
y_test <- prepped_glmnet1 %>% bake(all_outcomes(), composition='matrix', new_data=fake_test_data)
sqrt(mean((y_test - preds_glm6)^2))
```

# Trees

Common Packages

- `{rpart}`
- `{C50}`
- `{xgboost}`

## Recipe

```{r}
library(rsample)
train_test_split <- initial_split(data=med, prop=0.9, strata='charges')
train_test_split
test <- testing(train_test_split)
test
bulk <- training(train_test_split)
bulk
train_val_split <- initial_split(data=bulk, prop=0.9, strata='charges')
train_val_split
val <- testing(train_val_split)
train <- training(train_val_split)
val
train
```


```{r}
med
rec_xgboost1 <- recipe(charges ~ ., data=train) %>% 
    step_dummy(all_nominal(), one_hot=TRUE)
rec_xgboost1
```

```{r}
prepped_xgboost1 <- rec_xgboost1 %>% prep()
prepped_xgboost1
```

```{r}
train_x <- prepped_xgboost1 %>% juice(all_predictors(), composition='dgCMatrix')
train_y <- prepped_xgboost1 %>% juice(all_outcomes(), composition='dgCMatrix')

val_x <- prepped_xgboost1 %>% bake(all_predictors(), composition='dgCMatrix', new_data=val)
val_y <- prepped_xgboost1 %>% bake(all_outcomes(), composition='dgCMatrix', new_data=val)

test_x <- prepped_xgboost1 %>% bake(all_predictors(), composition='dgCMatrix', new_data=test)
test_y <- prepped_xgboost1 %>% bake(all_outcomes(), composition='dgCMatrix', new_data=test)
```

```{r}
library(xgboost)
train_xg <- xgb.DMatrix(data=train_x, label=train_y)
train_xg
val_xg <- xgb.DMatrix(data=val_x, label=val_y)
```

```{r}
colnames(train_x)
```

## Fitting

```{r}
cost7 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=1
)
cost7
summary(cost7)
```

```{r}
xgb.plot.multi.trees(cost7)
```

```{r}
cost8 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=1,
    watchlist=list(train=train_xg)
)
```

Decision Tree Drawbacks:

- Unstable (high variance)
- Overfitting
- Discontinuous functions (could be positive because it allows nonlinearity)
- Hard to interpret (for bigger trees)

Solutions:

- Random Forests (bagging: bootstrap averaging)
- Boosting

```{r}
cost9 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=100,
    watchlist=list(train=train_xg)
)
```

```{r}
xgb.plot.multi.trees(cost9)
```

```{r}
cost10 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=500,
    watchlist=list(train=train_xg)
)
```

```{r}
cost11 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=2500,
    watchlist=list(train=train_xg)
)
```

```{r}
cost12 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=100,
    watchlist=list(train=train_xg, validate=val_xg)
)
```

```{r}
library(dygraphs)
dygraph(cost12$evaluation_log)
cost12$evaluation_log %>% 
    .[validate_rmse == min(validate_rmse), ]
```

```{r}
cost13 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=100,
    early_stopping_rounds=30,
    watchlist=list(train=train_xg, validate=val_xg)
)
cost13$best_iteration
cost13$best_ntreelimit
cost13$best_score
cost13$best_msg
```

```{r}
cost13 %>% 
    xgb.importance(model=., trees=0:cost13$best_ntreelimit) %>% 
    xgb.plot.importance()

cost13 %>% 
    xgb.importance(model=., trees=0:cost13$best_ntreelimit) %>% 
    .[, sum(Gain)]
```

```{r}
cost14 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=100,
    early_stopping_rounds=30,
    watchlist=list(train=train_xg, validate=val_xg),
    max_depth=2
)
```

```{r}
cost15 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=100,
    early_stopping_rounds=30,
    watchlist=list(train=train_xg, validate=val_xg),
    max_depth=3
)
```

```{r}
cost16 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=100,
    early_stopping_rounds=30,
    watchlist=list(train=train_xg, validate=val_xg),
    max_depth=4
)
```

```{r}
cost17 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=100,
    early_stopping_rounds=30,
    watchlist=list(train=train_xg, validate=val_xg),
    max_depth=12
)
```

```{r}
cost18 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=100,
    early_stopping_rounds=30,
    watchlist=list(train=train_xg, validate=val_xg),
    max_depth=4,
    eta=0.1
)
```

```{r}
cost19 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=200,
    early_stopping_rounds=30,
    watchlist=list(train=train_xg, validate=val_xg),
    max_depth=4,
    eta=0.05
)
```

```{r}
cost20 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=200,
    early_stopping_rounds=30,
    watchlist=list(train=train_xg, validate=val_xg),
    max_depth=4,
    eta=0.3,
    subsample=0.5,
    colsample_bytree=0.5
)
```

```{r}
cost21 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=1,
    num_parallel_tree=300,
    early_stopping_rounds=30,
    watchlist=list(train=train_xg, validate=val_xg),
    max_depth=4,
    eta=0.3,
    subsample=0.5,
    colsample_bytree=0.5
)
```

```{r}
cost22 <- xgb.train(
    data=train_xg,
    objective='reg:squarederror',
    booster='gbtree',
    nrounds=100,
    num_parallel_tree=50,
    early_stopping_rounds=30,
    watchlist=list(train=train_xg, validate=val_xg),
    max_depth=4,
    eta=0.3,
    subsample=0.5,
    colsample_bytree=0.5
)
```

## Boosted Linear Model

```{r}
rec_xgboost2 <- recipe(charges ~ ., data=train) %>% 
    step_normalize(all_numeric(), -charges) %>% 
    step_dummy(all_nominal(), one_hot=TRUE) %>% 
    step_intercept()
```

```{r}
prepped_xgboost2 <- rec_xgboost2 %>% prep()
prepped_xgboost2
```

```{r}
# train x/y
# val x/y
train_lin_x <- prepped_xgboost2 %>% juice(all_predictors(), composition='dgCMatrix')
train_lin_y <- prepped_xgboost2 %>% juice(all_outcomes(), composition='matrix')

val_lin_x <- prepped_xgboost2 %>% bake(all_predictors(), composition='dgCMatrix', new_data=val)
val_lin_y <- prepped_xgboost2 %>% bake(all_outcomes(), composition='matrix', new_data=val)

train_lin_xg <- xgb.DMatrix(data=train_lin_x, label=train_lin_y)
val_lin_xg <- xgb.DMatrix(data=val_lin_x, label=val_lin_y)
```

